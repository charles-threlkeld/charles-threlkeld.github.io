<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-12-10 Sat 16:37 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="Chas Threlkeld" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org591ff2f">1. Introduction</a></li>
<li><a href="#orgeec301d">2. Defining Speech Acts</a>
<ul>
<li><a href="#org6a07a77">2.1. Philosophical Usage</a></li>
<li><a href="#orgc0943fc">2.2. Linguistic Usage</a></li>
<li><a href="#org5a6d71c">2.3. Psychological Usage</a></li>
<li><a href="#orgd64f149">2.4. Computational Usage</a></li>
<li><a href="#orgcb55f51">2.5. Goals of this work</a></li>
</ul>
</li>
<li><a href="#org256cc55">3. Descriptive Work</a>
<ul>
<li><a href="#org589c7b1">3.1. Definitions of Terms</a></li>
<li><a href="#orgcd955eb">3.2. Indirect Speech Act Frequency</a></li>
<li><a href="#orge526778">3.3. Indirect Speech Acts and FTOs in Conversation</a></li>
<li><a href="#orgad207ec">3.4. Turn Lengths</a></li>
<li><a href="#orgbc1510c">3.5. TRP Durations</a></li>
</ul>
</li>
<li><a href="#org6c0c0b1">4. Choosing Data,  Parameters, and Models</a>
<ul>
<li><a href="#orge4e4e29">4.1. Data</a></li>
<li><a href="#orge6fa60b">4.2. Parameters</a>
<ul>
<li><a href="#org130a318">4.2.1. Previous Speech Act</a></li>
<li><a href="#org5bea4c0">4.2.2. Previous TRP, Previous Speaker Switch</a></li>
<li><a href="#org9ad3ed4">4.2.3. Sentence Type Probability</a></li>
<li><a href="#orgf568518">4.2.4. TCU Length</a></li>
<li><a href="#org3020c67">4.2.5. Next TRP, Next Speaker-Switch</a></li>
<li><a href="#orgfef248a">4.2.6. Next Speech Act</a></li>
</ul>
</li>
<li><a href="#orgd5948c9">4.3. Models</a></li>
<li><a href="#orgc2f73cd">4.4. Metrics</a></li>
</ul>
</li>
<li><a href="#org33b5dad">5. Models and Iterations</a>
<ul>
<li><a href="#org927104a">5.1. Iterations</a></li>
<li><a href="#orgfebb645">5.2. Interpretation</a></li>
</ul>
</li>
<li><a href="#orgeb8d9b7">6. Conclusion and Future Work</a></li>
</ul>
</div>
</div>
<div id="outline-container-org591ff2f" class="outline-2">
<h2 id="org591ff2f"><span class="section-number-2">1.</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
Hello and welcome to my dissertation. We are taking the philosophical speech act and defining it computationally. We will look at the various problems that speech acts hope to solve, and how lenses in one discipline fall short in others. To address this, we build a speech act set from scratch, using the linguistic cues of conversation. This data-first approach side-steps descriptional approaches that might not apply to other situations, and is unique to this research.
</p>
</div>
</div>
<div id="outline-container-orgeec301d" class="outline-2">
<h2 id="orgeec301d"><span class="section-number-2">2.</span> Defining Speech Acts</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org6a07a77" class="outline-3">
<h3 id="org6a07a77"><span class="section-number-3">2.1.</span> Philosophical Usage</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Speech acts were first codified up by JL Austin and his student Searle. Their goal was to shore up a philosophy of language in which the spoken-word is truth-functional. Austin noticed that there are some speech acts which are true by virtue of being spoken (performatives), which means they are not simply descriptions of the world, but actions themselves &#x2014; speech acts. Searle took this idea and ran with it, dictating that there are six speech acts (&#x2026;list of acts&#x2026;). Each act had its own conditions for being uttered felicitously and its own effects on the truth values of the world around.
</p>

<p>
Here we see that the truth-function of the words we say is paramount.
</p>
</div>
</div>

<div id="outline-container-orgc0943fc" class="outline-3">
<h3 id="orgc0943fc"><span class="section-number-3">2.2.</span> Linguistic Usage</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The elephant in the room is Chomsky, but Chomsky himself claims to only work at the syntactic level, not semantic or pragmatic. Linguistics at large has still posited cognitive mechanisms in language that might be present to account for speech acts. Transformations, such as tag questions, change the structure of a sentence to create a differently inflected meaning than sentences not including these constructions. The difference between the two can be considered the action that they are performing. At its base, these linguistic analyses has given rise to the direct force hypothesis.
</p>

<p>
Talk about alternative approaches (e.g. Jackendoff's spatial language).
</p>

<p>
Here we see that the point of the speech act is to link the intention of speech to the words we say.
</p>
</div>
</div>

<div id="outline-container-org5a6d71c" class="outline-3">
<h3 id="org5a6d71c"><span class="section-number-3">2.3.</span> Psychological Usage</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Psychologists have operationalized speech acts as pragmatic differences for the ways that words are interpreted in contexts of conversation. Conversation analysis does this by studying sequences within conversation to see how it is organized. For example, adjacency pairs are types of speech that are often found bordering each other (e.g. offer/accept, thank/welcome). Other, more experimental psychologists have looked at speech acts to change behavior. Lena's study shows that speaker switch changes expectations about incoming speech. Gisladottir, BÃ¶gels and Levinson used context utterances to show that otherwise identical utterances are processed differently in the brain if their speech act is different.
</p>

<p>
Here we see that the speech act of the words is contextually dependent. 
</p>
</div>
</div>

<div id="outline-container-orgd64f149" class="outline-3">
<h3 id="orgd64f149"><span class="section-number-3">2.4.</span> Computational Usage</h3>
<div class="outline-text-3" id="text-2-4">
<p>
Computer scientists have been interested in speech acts for decades for use in conversational agents. Programming an agent to behave appropriately has turned out to be a tricky task. Developers realized that assuming that the literal words that people say is also what they mean leads to errors. Similarly, there are many ambiguities within language, and the speech act is one of these. A common approach to solving this problem is to create large corpuses of linguistic interactions tagged for speech acts. This leads to problems of which tag-set to use. DAMSL, ISO, SWBD<sub>DAMSL</sub>, Matthias's Big 5, Rhetorical Structure Theory and others have proposed to be solutions to speech act problems in agents, but each has fallen short in its own way, showing a disconnect between the motivation and the implementation of these projects.
</p>

<p>
Here we see that speech acts are for refining the interpretation of language or constraining the responses we get to our own utterances.
</p>
</div>
</div>

<div id="outline-container-orgcb55f51" class="outline-3">
<h3 id="orgcb55f51"><span class="section-number-3">2.5.</span> Goals of this work</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Each discipline has its own theory and use for speech acts. In this work, we want to draw on each of them in order to have a useful operationalization of speech acts. However, we take a data-first approach. We think that the psychological findings of Warnke &amp; Levinson are promising, but lack theoretical grounding for differentiating speech acts online. DAMSL, ISO, RST, etc., by contrast, all fall short by imposing their own constraints onto the data. Linguistic structure has not proven robust to the open-set of conversational utterances, but their mechanisms of encoding and decoding intention, still seems to be present for people.
</p>

<p>
Therefore, in this work, we look at meta-linguistic parameters within conversation to detail a speech act set and built a model for recognizing this set. This allows us to create a tag-set, ensure that we are operating at the pragmatic (rather than lexical, syntactic, or even semantic levels), and build from the reasoning that people do every day, rather than assign values of the reasoning we believe them to have done.
</p>
</div>
</div>
</div>

<div id="outline-container-org256cc55" class="outline-2">
<h2 id="org256cc55"><span class="section-number-2">3.</span> Descriptive Work</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org589c7b1" class="outline-3">
<h3 id="org589c7b1"><span class="section-number-3">3.1.</span> Definitions of Terms</h3>
<div class="outline-text-3" id="text-3-1">
<p>
In this and following chapters, we will use several terms that are used in different ways in different parts of the literature. Therefore, we outline definitions here for:
</p>
<ul class="org-ul">
<li>Speech Act</li>
<li>Direct Speech Act</li>
<li>Indirect Speech Act</li>
<li>Pragmatic Completion</li>
<li>Turn Construction Unit</li>
<li>Turn</li>
<li>Conversational Context</li>
</ul>
</div>
</div>
<div id="outline-container-orgcd955eb" class="outline-3">
<h3 id="orgcd955eb"><span class="section-number-3">3.2.</span> Indirect Speech Act Frequency</h3>
<div class="outline-text-3" id="text-3-2">
<p>
A re-telling of the CABNC work on indirect speech acts. A few hundred utterances were hand-tagged for sentence type (declarative, interrogative, imperative, none) and sentences were tagged for speech act based on a set motivated by sentence type (statement, question, command). The goal was to find how big of a problem indirect speech acts are in real-world conversation, even given a small set of speech acts. We found that indirect speech acts are somewhat common (~8%) and that non-sentence utterances were very common (~40%). We have an outline of where indirect speech acts are found and some ideas about how this study might be extended in future work.
</p>
</div>
</div>
<div id="outline-container-orge526778" class="outline-3">
<h3 id="orge526778"><span class="section-number-3">3.3.</span> Indirect Speech Acts and FTOs in Conversation</h3>
<div class="outline-text-3" id="text-3-3">
<p>
Paper published with Lena shows that FTOs do not differ in conjunction with being direct / indirect. The hypothesis is that if we did find a longer FTO for indirect speech acts, it could be evidence of extra cognitive processing. We did not find this, but we did find that the data was best explained by a model including only speech act. This suggests that the timing of conversation orients to the social work being done, not any supposed cognitive load. This is also evidence that the pragmatic level of conversation is separated from the syntactic level.
</p>
</div>
</div>
<div id="outline-container-orgad207ec" class="outline-3">
<h3 id="orgad207ec"><span class="section-number-3">3.4.</span> Turn Lengths</h3>
<div class="outline-text-3" id="text-3-4">
<p>
We look at 7 different formulation of Turn and TCU length and find that they are well described by exponential (or geometric) distributions. This means that, en masse, turns and TCUs have a constant hazard rate. We must therefore be projecting the end of turns by other means.
</p>
</div>
</div>
<div id="outline-container-orgbc1510c" class="outline-3">
<h3 id="orgbc1510c"><span class="section-number-3">3.5.</span> TRP Durations</h3>
<div class="outline-text-3" id="text-3-5">
<p>
TRP durations are sensitive to speaker switch. We found that in about 1/6 of cases, there was a 0ms TRP and the same speaker continued, 1/2 cases there was a speaker switch (TRP ~250ms) and 1/3 cases no speaker switch (TRP ~600ms). This shows that we are sensitive to what is being said when and by whom in conversation.
</p>
</div>
</div>
</div>
<div id="outline-container-org6c0c0b1" class="outline-2">
<h2 id="org6c0c0b1"><span class="section-number-2">4.</span> Choosing Data,  Parameters, and Models</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orge4e4e29" class="outline-3">
<h3 id="orge4e4e29"><span class="section-number-3">4.1.</span> Data</h3>
<div class="outline-text-3" id="text-4-1">
<p>
The data comes from the Switchboard corpus. We took data from the Jurafsky project that annotated with SWBD<sub>DAMSL</sub> tags and the MSU transcription that had timestamps down to the millisecond. We added our own annotations for sentence type using a DistilBERT model trained on a few hundred hand-tagged examples.
</p>
</div>
</div>
<div id="outline-container-orge6fa60b" class="outline-3">
<h3 id="orge6fa60b"><span class="section-number-3">4.2.</span> Parameters</h3>
<div class="outline-text-3" id="text-4-2">
<p>
In order of access in a conversation:
</p>
</div>
<div id="outline-container-org130a318" class="outline-4">
<h4 id="org130a318"><span class="section-number-4">4.2.1.</span> Previous Speech Act</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
A fundamental part of this idea is that our options are limited in response to previous context, therefore previous speech is perhaps the most important parameter of the models.
</p>
</div>
</div>
<div id="outline-container-org5bea4c0" class="outline-4">
<h4 id="org5bea4c0"><span class="section-number-4">4.2.2.</span> Previous TRP, Previous Speaker Switch</h4>
<div class="outline-text-4" id="text-4-2-2">
<p>
Based on previous work by Warnke, we expect that speech acts depend upon which speaker takes their turn next.
</p>

<p>
Previous work by the author shows that speaker-switch is time-sensitive. The pregnant pause paper and preference organization also make the argument that we are more likely to do some speech acts (e.g. accept, agree) faster than others (e.g. reject, disagree). Therefore, we include timing information, too.
</p>
</div>
</div>
<div id="outline-container-org9ad3ed4" class="outline-4">
<h4 id="org9ad3ed4"><span class="section-number-4">4.2.3.</span> Sentence Type Probability</h4>
<div class="outline-text-4" id="text-4-2-3">
<p>
Previous work shows that while it is not perfect, sentence type does inform the speech act in many cases. Our work here seeks a more fine-grained schema than sentence type provides, but that may simply segment the sentence types more finely.
</p>
</div>
</div>
<div id="outline-container-orgf568518" class="outline-4">
<h4 id="orgf568518"><span class="section-number-4">4.2.4.</span> TCU Length</h4>
<div class="outline-text-4" id="text-4-2-4">
<p>
Our work shows that TCU length is exponential, but since turn end is projectable, we suspect that there is some interaction between pragmatic closer and speech act.
</p>
</div>
</div>
<div id="outline-container-org3020c67" class="outline-4">
<h4 id="org3020c67"><span class="section-number-4">4.2.5.</span> Next TRP, Next Speaker-Switch</h4>
<div class="outline-text-4" id="text-4-2-5">
<p>
While next TRP and Next Speaker-switch are not available during the turn itself, the reaction to an utterance may be an important part of its characterization. For example, a rhetorical question may be followed by speaker continuation while an inquiry would be followed by a speaker switch. We include TRP for similar reasons as above.
</p>
</div>
</div>
<div id="outline-container-orgfef248a" class="outline-4">
<h4 id="orgfef248a"><span class="section-number-4">4.2.6.</span> Next Speech Act</h4>
<div class="outline-text-4" id="text-4-2-6">
<p>
While our chief goal is to understand how responses are constrained by context, it is also important what constraints are created. This is characterized by following speech act.
</p>
</div>
</div>
</div>
<div id="outline-container-orgd5948c9" class="outline-3">
<h3 id="orgd5948c9"><span class="section-number-3">4.3.</span> Models</h3>
<div class="outline-text-3" id="text-4-3">
<p>
We looked into several models for clustering: Affinity Propagation, Mean Shift, DBSCAN, and OPTICS. Each has its own advantages and drawbacks, and since we weren't sure what the geometry of the problem is that we are addressing, we pursued each with the metrics listed below.
</p>
</div>
</div>
<div id="outline-container-orgc2f73cd" class="outline-3">
<h3 id="orgc2f73cd"><span class="section-number-3">4.4.</span> Metrics</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Metrics had two types: known ground truth and unknown ground truth. The advantage of known truth was that they worked for all models. The disadvantage was that we know the ground truth had issues. The problem with the metrics that had an unknown ground truth is that they do not play well with the DBSCAN and OPTICS algorithms due to the geometry of those solutions. However, DBSCAN and OPTICS do come with their own metrics that determine whether points are core or non-core for their clusters, which gives a sense for how well-clustered the data is in the model, which is useful in this context. 
</p>
</div>
</div>
</div>
<div id="outline-container-org33b5dad" class="outline-2">
<h2 id="org33b5dad"><span class="section-number-2">5.</span> Models and Iterations</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org927104a" class="outline-3">
<h3 id="org927104a"><span class="section-number-3">5.1.</span> Iterations</h3>
<div class="outline-text-3" id="text-5-1">
<p>
While there is considerable debate about the number of speech acts that is appropriate to a schema, our goal was about a dozen. Our thinking is informed by examining the list of acts in competing schemas and background research on preference organization. There were several different types of statement, question, command, and non-sentence utterances that we thought a dozen speech acts would cover without any individual being too rare.
</p>

<p>
Therefore, we iterated on our clustering algorithm X times, reducing the number of labels in each iteration. Since the accessible, ground-truth parameters were consistent throughout, they should allow us to maintain consistency across iterations. However, since there are more parameters (due to the one-hot encoding) pointing to the previous (and next) labels, this is still a major feature of the model
</p>
</div>
</div>
<div id="outline-container-orgfebb645" class="outline-3">
<h3 id="orgfebb645"><span class="section-number-3">5.2.</span> Interpretation</h3>
<div class="outline-text-3" id="text-5-2">
<p>
The clusters had parameters X, Y, and Z.
</p>

<p>
In practice, this meant that the algorithm had clustered around utterances like A B and C.
</p>

<p>
As some examples, we see that label N has words like W1, W2, W3, even though we did not train on words. Here at the end of the work, we can finally add post-hoc descriptive labels to the clusters. Our interpretation of these labels is as follows:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">L1</td>
<td class="org-left">Desc1</td>
</tr>

<tr>
<td class="org-left">L2</td>
<td class="org-left">Desc2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="outline-container-orgeb8d9b7" class="outline-2">
<h2 id="orgeb8d9b7"><span class="section-number-2">6.</span> Conclusion and Future Work</h2>
<div class="outline-text-2" id="text-6">
<p>
This work attempts to re-situate speech acts in conversation into the data of conversation, rather than descriptive analysis. The work shows that we can create a model of conversation where each utterance constrains the next utterance in certain normative, predictable ways. Our model orients to these constraints and builds a set of speech acts based on conversational behavior, which can be used to improve the naturalness of agents, and also to explore how intersubjectivity is maintained in conversation despite ambiguity.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Chas Threlkeld</p>
<p class="date">Created: 2022-12-10 Sat 16:37</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
